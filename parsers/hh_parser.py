import requests
from bs4 import BeautifulSoup
from .base_parser import BaseParser
from typing import List, Dict
import time


class HHParser(BaseParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è —Å–∞–π—Ç–∞ HH.ru"""

    def __init__(self):
        super().__init__('hh')
        self.base_url = 'https://hh.ru'
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }

    def search(self, query: str, limit: int = 20, city: str = '') -> List[Dict]:
        """–ü–æ–∏—Å–∫ –≤–∞–∫–∞–Ω—Å–∏–π –Ω–∞ HH.ru —Å —Ñ–∏–ª—å—Ç—Ä–æ–º –ø–æ –≥–æ—Ä–æ–¥—É"""
        print(f"üîç –ü–æ–∏—Å–∫ –Ω–∞ HH.ru: {query}" + (f" –≤ –≥–æ—Ä–æ–¥–µ {city}" if city else ""))
        vacancies = []

        try:
            # –§–æ—Ä–º–∏—Ä—É–µ–º URL –¥–ª—è –ø–æ–∏—Å–∫–∞
            search_url = f'{self.base_url}/search/vacancy'
            params = {
                'text': query,
                'per_page': min(limit, 50)
            }

            # –î–û–ë–ê–í–ò–¢–¨ –û–ë–†–ê–ë–û–¢–ö–£ –ì–û–†–û–î–ê:
            if city:
                params['area'] = city
                print(f"üìç –ü—Ä–∏–º–µ–Ω–µ–Ω —Ñ–∏–ª—å—Ç—Ä –ø–æ –≥–æ—Ä–æ–¥—É ID: {city}")

            response = requests.get(search_url, headers=self.headers, params=params, timeout=10)
            response.raise_for_status()

            soup = BeautifulSoup(response.text, 'html.parser')

            # –ò—â–µ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã —Å –≤–∞–∫–∞–Ω—Å–∏—è–º–∏
            vacancy_items = soup.find_all('div', {'data-qa': 'vacancy-serp__vacancy'})

            for item in vacancy_items[:limit]:
                try:
                    vacancy_data = self._parse_vacancy_item(item)
                    if vacancy_data:
                        vacancies.append(vacancy_data)
                        self.save_vacancy(vacancy_data)

                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤–∞–∫–∞–Ω—Å–∏–∏ HH: {e}")
                    continue

                # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(0.3)

        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–∞ HH.ru: {e}")

        print(f"‚úÖ HH.ru: –Ω–∞–π–¥–µ–Ω–æ {len(vacancies)} –≤–∞–∫–∞–Ω—Å–∏–π")
        return vacancies

    def _parse_vacancy_item(self, item) -> Dict:
        """–ü–∞—Ä—Å–∏–Ω–≥ –æ—Ç–¥–µ–ª—å–Ω–æ–π –≤–∞–∫–∞–Ω—Å–∏–∏"""
        # –ù–∞–∑–≤–∞–Ω–∏–µ –∏ —Å—Å—ã–ª–∫–∞
        title_tag = item.find('a', {'data-qa': 'serp-item__title'})
        if not title_tag:
            title_tag = item.find('a', class_=lambda x: x and 'magritte-link' in x)

        if not title_tag:
            return None

        title = title_tag.text.strip()
        link = title_tag['href']
        if not link.startswith('http'):
            link = self.base_url + link

        # –ö–æ–º–ø–∞–Ω–∏—è
        company_tag = item.find('a', {'data-qa': 'vacancy-serp__vacancy-employer'})
        if not company_tag:
            company_tag = item.find('div', {'data-qa': 'vacancy-serp__vacancy-employer'})
        company = company_tag.text.strip() if company_tag else '–ù–µ —É–∫–∞–∑–∞–Ω–∞'

        # –ó–∞—Ä–ø–ª–∞—Ç–∞
        salary_tag = item.find('span', {'data-qa': 'vacancy-serp__vacancy-compensation'})
        salary = salary_tag.text.strip() if salary_tag else '–ù–µ —É–∫–∞–∑–∞–Ω–∞'

        return {
            'title': title,
            'link': link,
            'company': company,
            'salary': salary,
            'source': 'hh'
        }